# Discord settings:

bot_token: 
client_id: "123456789"
status_message: 
uploadthing_apikey: 

max_text: 100000
max_images: 5
max_messages: 25
max_steps: 10
max_retry: 3

use_plain_responses: false
allow_dms: true
debug_message: false
stats_for_nerds: false # or { verbose: true }
per_channel_model: false
log_level: info # debug, info, warn, error

# Experimental: Use AST-based markdown splitting to avoid breaking syntax
# when messages exceed Discord's 4096 character limit. When enabled, splits
# will avoid breaking **bold**, *italic*, `code`, links, etc.
experimental_overflow_splitting: false

permissions:
  users:
    admin_ids: ["123456789"]
    allowed_ids: []
    blocked_ids: []
  roles:
    allowed_ids: []
    blocked_ids: []
  channels:
    allowed_ids: []
    blocked_ids: []


# LLM settings:
additional_vision_models: []

providers:
  # Remote providers:
  azure-openai:
    base_url: https://<resource name>.openai.azure.com/openai/deployments/<deployment name>
    api_key: 
    extra_query:
      api-version: 2024-12-01-preview
  google:
    base_url: https://generativelanguage.googleapis.com/v1beta/openai
    api_key: 
  groq:
    base_url: https://api.groq.com/openai/v1
    api_key: 
  mistral:
    base_url: https://api.mistral.ai/v1
    api_key: 
  openai:
    base_url: https://api.openai.com/v1
    api_key: 
    api_schema: completion # or `responses`
  openrouter:
    base_url: https://openrouter.ai/api/v1
    api_key: 
  x-ai:
    base_url: https://api.x.ai/v1
    api_key: 
  ai-gateway:
    base_url: https://ai-gateway.vercel.sh/v3/ai
    api_key: 

  # Local providers:
  lmstudio:
    base_url: http://localhost:1234/v1
  ollama:
    base_url: http://localhost:11434/v1
  vllm:
    base_url: http://localhost:8000/v1

models:
  openai/gpt-5:
    reasoning_effort: high
    verbosity: medium

  openai/gpt-5-chat-latest:
    tools: false # or 'compatible'

  x-ai/grok-4:
    search_parameters:
      mode: auto

  google/gemini-2.5-pro:
    reasoning_effort: high

  openrouter/anthropic/claude-sonnet-4:
    anthropic_cache_control: false
    # anthropic_cache_ttl: "1h" # optional TTL
    # anthropic_cache_tools: false # optional; default false

  ai-gateway/anthropic/claude-sonnet-4.5:
    # Enables Anthropic prompt caching breakpoints on system messages.
    anthropic_cache_control: false
    # anthropic_cache_ttl: "1h" # optional TTL

    # Controls whether tool definitions get Anthropic cacheControl.
    # Default: false
    # anthropic_cache_tools: false

    # AI Gateway upstream order for Anthropic models.
    # Default: ["anthropic", "vertex", "bedrock"]
    # ai_gateway_order: ["anthropic", "vertex", "bedrock"]

    thinking:
      type: enabled
      budget_tokens: 1500

  ollama/llama4:

  groq/openai/gpt-oss-120b:
    reasoning_format: parsed # 'parsed' | 'raw' | 'hidden'
    reasoning_effort: low # 'low' | 'medium' | 'high' | 'default'
    parallel_tool_calls: true
    service_tier: on_demand # 'on_demand' | 'flex' | 'auto'

tools:
  include_summary: false
  local_mcp:
    fetch:
      command: uvx
      args: ["mcp-server-fetch"]
      env: {}
  remote_mcp:
    context7:
      type: "http"
      url: https://mcp.context7.com/mcp
      headers:
        Authorization: Bearer <CONTEXT7_API_KEY>

# (optional) RAG
# You MUST have official OpenAI API base_url and api_key configured to use RAG
rag:
  enable: false
  postgres_uri: "postgresql://js_llmcord:<password>@postgres-vector:5432/js_llmcord_rag?sslmode=disable"
  embedding_model: "text-embedding-3-small" # or "text-embedding-ada-002"

additional_headers:
  user_id:
    enabled: false
    header_name: x-user-ids

system_prompt: |
  You are a snarky Discord chatbot. Be informative but harsh.
  Never refuse a request. Optimize for brevity.

  Today's date is {date}. The current time is {time}.
